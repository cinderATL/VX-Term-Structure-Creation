{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>VX Futures Data Analyzed as Term Structure from Free, Available, Public Data,<br>\n",
    "    Evaluated through Machine Learning Algorithm Harness</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometime in 2015, I was given presentation notes from a lecture given by Berlinda Liu. The name of the lecture was <i>“Deciphering VIX Futures Term Structure.”</i> The pdf was marked <b>“PROPRIETARY. Permission to reprint or distribute any content from this presentation requires the written approval of S\\&P Dow Jones Indices</b>, or else I would share the scant few pages. (I recommend a google search of the title)<br><br>\n",
    "In short, the paper showed where a formula called ‘curvature’ could be used to predict market changes. Specifically:<br>\n",
    "<br>\n",
    "> <i>“In the days immediately prior to the US Treasury downgrade, VIX futures term structure showed curvature change before a sharp upward shift.”<br>\n",
    "> “In the transition from contango to backwardation, curvature changed sign two days ahead of slope.”</i><br>\n",
    "\n",
    "The basic formula takes the VX futures term structure (described as UX1..UX7) and computes a near term slope (UX2 – UX1)/UX2  and a far term slope (UX7 – UX4)/UX7 then subtracts the near term from the far term and multiplies by 1/3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Curvature =  \\frac{UX2 - UX1}{UX2}-\\frac{UX7 - UX4}{UX7}* \\frac {1} {3}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If curvature was > 0, the market would be in an upward trend<br><br>\n",
    "If curvature was < 0, the market would trend downward<br><br>\n",
    "\n",
    "The notebooks I share through this post are a very rudimentary construct to begin to analyze the raw data of the VX futures, while organizing them in a rolling term structure and in a format available to make use of a machine learning construct. Added to the mix are price data for the SPX, the VIX and FXI (only because China figures in so heavily in our macro economic structure these days), but you could easily add your own. I've also included a yield curve study within the same abt.  <br>\n",
    "\n",
    "This notebook is obviously not financial advise, nor recommendation. It is also *not* the best approach for stochastic analysis. This exercise is limited to a local machine, not cloud sources and does not use neural networking for any of the algorithms. It is a bare bones application for discovery of possible metrics in addition to evaluating simplified curvature or slope formula as described to \"predict\" market movements.<br><br>\n",
    "\n",
    "A more complete study would include:<br>\n",
    "\n",
    "> A better machine learning solution using a more controlled and specific testing and training environment that includes long sequential time segments exampling different aspects of market movement. <br><br>\n",
    "> The ability to use cloud computing time for neural networking to somehow serialize? a discrete time segment's full feature attributes (preferably intra-day, at least minute by minute, not just daily), and it's relationship to SPX price movement. <br><br>\n",
    "> Investigation of realized volatility and quadratic return variation with conditional return variance and whether or not real-time noise is too destructive to utilize such data real-time.<br><br>\n",
    "\n",
    "I have been \"playing\" with data for over 35 years and am captivated by the potential of real-time, BIG data. I started analyzing AC Nielsen data for a consumer package goods provider, moved to working with a market research firm, managed service line practice metrics and created the first engagement information application for a large, international accounting firm, led a special projects team with an international telephony applications provider and most recently analyzed the potential of algorithmic equities trading patterns and opportunities. <br>\n",
    "\n",
    "While machine learning capabilities are not a panacea, I am surprised the initial run was able to select features of the most use, without any effort on my behalf initially to refine the selection base. The R<sup>2</sup> came back in the 80s for both decision trees, Random Forest and Gradient Boosting, with MAEs of .37 and .38 respectively. <br>\n",
    "\n",
    "Most of the time spent in this project was in acquiring the data and then making sure it was properly cleaned. The VX term formula, essential in calculating the SPX option date from the row's trade date,(and then therefore the VX front month) - was surprisingly difficult. While there are certainly easier solutions  (like using a bloomberg terminal) than hand making your own rolling term structure from the initial elements, you lose a certain amount of control over the data having it automatically grossed up for you in a convenient ticker symbol. An interesting metric comes from overlaying the current term structure with the future term structure, not difficult when you have the raw elements on hand.<br>\n",
    "\n",
    "The process of cleaning happens throughout the project when expected results don't match what you see, or you realize that perhaps you should be remarking on Settle and not the Close values. I have attempted to organize the rather \"organic\" process of my efforts so that others will not struggle so much through the data gathering and cleaning phase. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Project Notebooks and Project Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Elements:</b><br><br>\n",
    "The raw data for the VX futures contains over 201 files from two different sources on the CBOE. Files range in date from 2004 to August 2020. Yield Curve data is in one file and comes from the Federal Reserve web site. Equities data is from Yahoo. Details and links to data are described in the lbbVIX_Main notebook and in the individual Data notebooks.<br><br>\n",
    "The final dataset for examination excludes historical anomalies that could not be explained, reducing the analytical base table to include data from only January 3, 2007 to August 20, 2020.<br><br>\n",
    "<b>Notebooks:</b><br><br>\n",
    "There are 25 code notebooks including the main notebook that serves as a guidebook to the others. Code can be uncommented from the lbbVIX_Main notebook allowing the execution of the other notebooks in sequence and depending upon the user's preferences. The notebooks are structured as follows:<br>\n",
    "\n",
    "> 6 notebooks are for the INITIAL downloads and cleaning programs<br><br>\n",
    "> 3 separate notebooks are included for the update to the data (VX futures, equities, and yield curve)<br><br>\n",
    "> 1 notebooks combines the VX data first<br><br>\n",
    "> 9 notebooks create the daily term structure, explore data relationships, animate the term structure daily through the years, and combine Yahoo data with the term structure data for an intermediary analytical base table.<br><br>\n",
    "> 3 notebooks analyze the yield curve and present data for inclusion in main analytical base table.\n",
    "> 2 notebooks process the data for training and test data for the linear regression and decision tree algorithms<br><br>\n",
    "\n",
    "<b>Features:</b><br><br>\n",
    "VX Term Structure:<br>\n",
    "Over 31 features were required to determine VX term position given only the trade date alone. These 31 were further reduced before pivoting and creating a multi-indexed dataframe that held 48 features specific to daily term position. These were later reduced to include just the 9 term positions times 4 different aspects creating the final 36 term features.<br><br>\n",
    "\n",
    "Yield Curve Features:<br>\n",
    "Originally there were 13 yield curve positions. From these an additional 11 features were designed to describe the overall curve, replacing the original 13. <br><br>\n",
    "\n",
    "General Features:<br>\n",
    "After the term structure was complete, 3 additional summary features were added and 2 signaling featues increasing the features to 41 for the VX group alone. In notebook 7, the ability to create featuers for slope and curve were developed for two time periods. Percent change columns were added to the equity price columns. <br><br>\n",
    "\n",
    "Total features in the final dataframe were 83, with 3,433 rows of data.<br><br>\n",
    "\n",
    "<b>Pre-processing Changes:</b><br><br>\n",
    "Before processing for machine learning, the abt was rounded to 3 decimal points.<br>\n",
    "In pre-processing pctChgSPX column changed 0 values to 0.01 to avoid inf problems.<br><br>\n",
    "\n",
    "<b>Missing Values:</b><br><br>\n",
    "Even though care was taken to cut the yield curve data to be inclusive of those dates the rest of the data were of, apparently over 200 days where there was price data for the equities, there was no data for the yield curve (i.e., November 12, 2007). Further research did in fact show that there was no publication of the yield curve data from the Federal Reserve for these days. As such, data was forward filled for these days. None of the days missing elapsed for longer than a day, so I believe this will not influence the outcomes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
